{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bf2d09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Pushkar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Pushkar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Pushkar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import gc\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import PorterStemmer \n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "import pickle\n",
    "import regex as re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2ce737e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_cat(text):\n",
    "    if text=='unk_cat':\n",
    "        return [\"No Label\", \"No Label\", \"No Label\"]\n",
    "    return text.split(\"/\")\n",
    "\n",
    "def split_categories(df):\n",
    "    \"\"\"\n",
    "    Desription:\n",
    "    Separate categories into three parts\n",
    "\n",
    "    Input: Dataframe having category_name field\n",
    "    Output: Dataframe with splitted categories\n",
    "    \"\"\"\n",
    "    df['general_cat'], df['subcat_1'], df['subcat_2'] = zip(*df['category_name'].apply(lambda x: split_cat(x)))\n",
    "    df = df.drop('category_name', axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40d5b5fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>item_condition_id</th>\n",
       "      <th>category_name</th>\n",
       "      <th>brand_name</th>\n",
       "      <th>price</th>\n",
       "      <th>shipping</th>\n",
       "      <th>seller_id</th>\n",
       "      <th>item_description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17</td>\n",
       "      <td>Hold Alyssa Frye Harness boots 12R, Sz 7</td>\n",
       "      <td>3</td>\n",
       "      <td>Women/Shoes/Boots</td>\n",
       "      <td>Frye</td>\n",
       "      <td>79</td>\n",
       "      <td>1</td>\n",
       "      <td>211140753</td>\n",
       "      <td>Good used condition Women's Fyre harness boots...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19</td>\n",
       "      <td>Steve Madden booties</td>\n",
       "      <td>3</td>\n",
       "      <td>Women/Shoes/Boots</td>\n",
       "      <td>Steve Madden</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>3874531266</td>\n",
       "      <td>The brand is actually \"Steven\" by Steve Madden...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>42</td>\n",
       "      <td>BCBG Tan Booties</td>\n",
       "      <td>1</td>\n",
       "      <td>Women/Shoes/Boots</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>8341537216</td>\n",
       "      <td>Brand new! Does not include the box.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45</td>\n",
       "      <td>NWT Sorel Caribou boots size 8.5</td>\n",
       "      <td>1</td>\n",
       "      <td>Women/Shoes/Boots</td>\n",
       "      <td>NaN</td>\n",
       "      <td>85</td>\n",
       "      <td>0</td>\n",
       "      <td>4040379892</td>\n",
       "      <td>New in box. Size 8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>58</td>\n",
       "      <td>NIB Hunter Tiffany Mint Boots Size 5</td>\n",
       "      <td>1</td>\n",
       "      <td>Women/Shoes/Boots</td>\n",
       "      <td>Hunter</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>19216599</td>\n",
       "      <td>Brand new never worn only flaw is as you can s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                      name  item_condition_id  \\\n",
       "0  17  Hold Alyssa Frye Harness boots 12R, Sz 7                  3   \n",
       "1  19                      Steve Madden booties                  3   \n",
       "2  42                          BCBG Tan Booties                  1   \n",
       "3  45          NWT Sorel Caribou boots size 8.5                  1   \n",
       "4  58      NIB Hunter Tiffany Mint Boots Size 5                  1   \n",
       "\n",
       "       category_name    brand_name  price  shipping   seller_id  \\\n",
       "0  Women/Shoes/Boots          Frye     79         1   211140753   \n",
       "1  Women/Shoes/Boots  Steve Madden     31         0  3874531266   \n",
       "2  Women/Shoes/Boots           NaN     48         0  8341537216   \n",
       "3  Women/Shoes/Boots           NaN     85         0  4040379892   \n",
       "4  Women/Shoes/Boots        Hunter    200         0    19216599   \n",
       "\n",
       "                                    item_description  \n",
       "0  Good used condition Women's Fyre harness boots...  \n",
       "1  The brand is actually \"Steven\" by Steve Madden...  \n",
       "2               Brand new! Does not include the box.  \n",
       "3                               New in box. Size 8.5  \n",
       "4  Brand new never worn only flaw is as you can s...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv('mercari_train.csv')\n",
    "\n",
    "# Price range should be $1-$2000, https://www.mercari.com/us/help_center/article/69\n",
    "train_df = train_df[train_df['price']>=1]  # Removing the prodicts with prices less than 1\n",
    "train_df = train_df[train_df['price']<=2000]  # Removing the prodicts with prices more than 2000\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acdfdb2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Women \n",
      "-------------\n",
      "90 th percentile is 35.0\n",
      "91 th percentile is 39.620000000000005\n",
      "92 th percentile is 43.0\n",
      "93 th percentile is 45.0\n",
      "94 th percentile is 50.0\n",
      "95 th percentile is 55.0\n",
      "96 th percentile is 62.860000000000014\n",
      "97 th percentile is 70.0\n",
      "98 th percentile is 84.3599999999999\n",
      "99 th percentile is 163.51000000000124\n",
      "100 th percentile is 725.0\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>item_condition_id</th>\n",
       "      <th>brand_name</th>\n",
       "      <th>price</th>\n",
       "      <th>shipping</th>\n",
       "      <th>seller_id</th>\n",
       "      <th>item_description</th>\n",
       "      <th>general_cat</th>\n",
       "      <th>subcat_1</th>\n",
       "      <th>subcat_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17</td>\n",
       "      <td>Hold Alyssa Frye Harness boots 12R, Sz 7</td>\n",
       "      <td>3</td>\n",
       "      <td>Frye</td>\n",
       "      <td>79</td>\n",
       "      <td>1</td>\n",
       "      <td>211140753</td>\n",
       "      <td>Good used condition Women's Fyre harness boots...</td>\n",
       "      <td>Women</td>\n",
       "      <td>Shoes</td>\n",
       "      <td>Boots</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19</td>\n",
       "      <td>Steve Madden booties</td>\n",
       "      <td>3</td>\n",
       "      <td>Steve Madden</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>3874531266</td>\n",
       "      <td>The brand is actually \"Steven\" by Steve Madden...</td>\n",
       "      <td>Women</td>\n",
       "      <td>Shoes</td>\n",
       "      <td>Boots</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>42</td>\n",
       "      <td>BCBG Tan Booties</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>8341537216</td>\n",
       "      <td>Brand new! Does not include the box.</td>\n",
       "      <td>Women</td>\n",
       "      <td>Shoes</td>\n",
       "      <td>Boots</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45</td>\n",
       "      <td>NWT Sorel Caribou boots size 8.5</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>85</td>\n",
       "      <td>0</td>\n",
       "      <td>4040379892</td>\n",
       "      <td>New in box. Size 8.5</td>\n",
       "      <td>Women</td>\n",
       "      <td>Shoes</td>\n",
       "      <td>Boots</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>58</td>\n",
       "      <td>NIB Hunter Tiffany Mint Boots Size 5</td>\n",
       "      <td>1</td>\n",
       "      <td>Hunter</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>19216599</td>\n",
       "      <td>Brand new never worn only flaw is as you can s...</td>\n",
       "      <td>Women</td>\n",
       "      <td>Shoes</td>\n",
       "      <td>Boots</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                      name  item_condition_id  \\\n",
       "0  17  Hold Alyssa Frye Harness boots 12R, Sz 7                  3   \n",
       "1  19                      Steve Madden booties                  3   \n",
       "2  42                          BCBG Tan Booties                  1   \n",
       "3  45          NWT Sorel Caribou boots size 8.5                  1   \n",
       "4  58      NIB Hunter Tiffany Mint Boots Size 5                  1   \n",
       "\n",
       "     brand_name  price  shipping   seller_id  \\\n",
       "0          Frye     79         1   211140753   \n",
       "1  Steve Madden     31         0  3874531266   \n",
       "2           NaN     48         0  8341537216   \n",
       "3           NaN     85         0  4040379892   \n",
       "4        Hunter    200         0    19216599   \n",
       "\n",
       "                                    item_description general_cat subcat_1  \\\n",
       "0  Good used condition Women's Fyre harness boots...       Women    Shoes   \n",
       "1  The brand is actually \"Steven\" by Steve Madden...       Women    Shoes   \n",
       "2               Brand new! Does not include the box.       Women    Shoes   \n",
       "3                               New in box. Size 8.5       Women    Shoes   \n",
       "4  Brand new never worn only flaw is as you can s...       Women    Shoes   \n",
       "\n",
       "  subcat_2  \n",
       "0    Boots  \n",
       "1    Boots  \n",
       "2    Boots  \n",
       "3    Boots  \n",
       "4    Boots  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expensive_brands = []\n",
    "train_exp = split_categories(train_df)\n",
    "unique_gen_cat = train_exp['general_cat'].unique()\n",
    "cat_threshold_percetile = {}\n",
    "for gc_ in unique_gen_cat: \n",
    "    print(gc_,\"\\n-------------\")\n",
    "    for i in range(90,101):\n",
    "        print(i,\"th percentile is\",np.percentile(train_df[train_exp['general_cat']==gc_].groupby('brand_name').median().sort_values(\"price\", ascending=False)['price'].reset_index()['price'].values, i))\n",
    "    cat_threshold_percetile[gc_]=np.percentile(train_exp[train_df['general_cat']==gc_].groupby('brand_name').median().sort_values(\"price\", ascending=False)['price'].reset_index()['price'].values, 95)\n",
    "    print()\n",
    "for gc_ in unique_gen_cat:\n",
    "    temp_df = train_exp[train_df['general_cat']==gc_][['brand_name','price']].values\n",
    "    for row in temp_df:\n",
    "        if row[1]>=cat_threshold_percetile[gc_]:\n",
    "            expensive_brands.append(row[0])\n",
    "expensive_brands = set(expensive_brands)\n",
    "train_exp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e815f265",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['log_price'] = np.log(train_df['price'].values)\n",
    "X = train_df.drop(['id','log_price','price'], axis=1)\n",
    "y = train_df['log_price']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "del X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "012e5848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54720, 10)\n",
      "(54720,)\n",
      "(13680, 10)\n",
      "(13680,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4296e031",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_values(df):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    Filling null values in brand_name\n",
    "\n",
    "    Input: Dataframe with null values\n",
    "    Output: Dataframe with no null values\n",
    "    \"\"\"\n",
    "    df['brand_name'].fillna('unk_brand', inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37e79bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def decontract_text(phrase):\n",
    "    \"\"\"\n",
    "    This utility funciton will be used as a part of preprocessing the text.\n",
    "    It will expand the contracted words. For eg: won't -> will not, I'm -> I am.\n",
    "    \"\"\"\n",
    "    phrase = str(phrase)\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase\n",
    "\n",
    "def stem_sentence(sentence):\n",
    "    ps = PorterStemmer()\n",
    "    words = word_tokenize(sentence)\n",
    "    root = []\n",
    "    for w in words: \n",
    "        root.append(ps.stem(w))\n",
    "    return \" \".join(root)\n",
    "\n",
    "def preprocess_descriptive_text_column(sentance):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    This function will process the text data.\n",
    "    This function will perform decontracting words, removing stop words, removing special characters and then apply stemming on the words in the sentence.\n",
    "\n",
    "    Input: original sentence\n",
    "    Output: processed sentence\n",
    "    \"\"\"\n",
    "    # https://gist.github.com/sebleier/554280\n",
    "    # we are removing the negative words from the stop words list: 'no', 'nor', 'not', 'shouldn't, won't, etc.\n",
    "    stopwords= ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
    "                \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
    "                'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
    "                'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
    "                'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
    "                'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
    "                'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
    "                'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
    "                'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
    "                'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
    "                's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
    "                've', 'y']\n",
    "\n",
    "    sent = decontract_text(sentance)\n",
    "    sent = sent.replace('\\\\r', ' ')\n",
    "    sent = sent.replace('\\\\n', ' ')\n",
    "    sent = sent.replace('\\\\\"', ' ')\n",
    "    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
    "    # https://gist.github.com/sebleier/554280\n",
    "    sent = ' '.join(e for e in sent.split() if e.lower() not in stopwords)\n",
    "\n",
    "    root_sent = stem_sentence(sent.lower().strip())\n",
    "    return root_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7087ebcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_categories(x):\n",
    "        return set(x.values)\n",
    "\n",
    "def brand_guesser(df):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    This function is used to guess the missing brand name.\n",
    "\n",
    "    Inputs: dataframe with missing brand names\n",
    "    Output: dataframe with filled brand names\n",
    "    \"\"\"\n",
    "    existing_brands = df[df['brand_name'] != 'unk_brand']['brand_name'].unique()\n",
    "    brand_names_categories = dict(df[df['brand_name'] != 'unk_brand'][['brand_name','category_name']].astype('str').groupby('brand_name').agg(concat_categories).reset_index().values.tolist())\n",
    "    filled_brands = []\n",
    "    for row in tqdm(df[['brand_name','name','category_name']].values):\n",
    "        found=False\n",
    "        if row[0]=='unk_brand':\n",
    "            for brand in existing_brands:\n",
    "                if brand in row[1] and row[2] in brand_names_categories[brand] :\n",
    "                    filled_brands.append(brand)\n",
    "                    found=True\n",
    "                    break\n",
    "            if not found:\n",
    "                filled_brands.append('unk_brand')\n",
    "        else:\n",
    "            filled_brands.append(row[0])\n",
    "\n",
    "    df['brand_name']=filled_brands\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "652899ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "def get_len_feature(col_series, scaler_text_len=None):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    This funciton will calculate the word count of the text and standardize it.\n",
    "\n",
    "    Input: Series, fitted scaler[optional; used during inference]\n",
    "    Output: standardized text length for each product and object of the fitted scaler\n",
    "    \"\"\"\n",
    "    text_len = col_series.apply(lambda x: len(x.split()))\n",
    "    if scaler_text_len==None:\n",
    "        scaler_text_len = StandardScaler()\n",
    "        scaler_text_len.fit(text_len.values.reshape(-1, 1))\n",
    "    text_len = scaler_text_len.transform(text_len.values.reshape(-1, 1))\n",
    "    return text_len, scaler_text_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f09b8910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shipping_feature(df):\n",
    "    sparse_shipping = scipy.sparse.csr_matrix(df['shipping'].values)\n",
    "    sparse_shipping = sparse_shipping.reshape(-1,1) # Now the shape will be (1111901, 1)\n",
    "    return sparse_shipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dad74bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_is_expensive_feature(df):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    This funciton will generate a feature which will tell if the brand is expensive or not.\n",
    "\n",
    "    Input: Dataframe\n",
    "    Output: Sparse is_expensive data\n",
    "    \"\"\"\n",
    "    \n",
    "    is_expensive_binary = df['brand_name'].apply(lambda x: 1 if x in expensive_brands else 0)\n",
    "    sparse_shipping = scipy.sparse.csr_matrix(is_expensive_binary.values)\n",
    "    sparse_shipping = sparse_shipping.reshape(-1,1) # Now the shape will be (1111901, 1)\n",
    "    return sparse_shipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34b99693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_data(col_data, vectorizer=None):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "    This funciton will vectorize the input column data.\n",
    "\n",
    "    Input: dataframe column\n",
    "    Output: one-hot encoded values and the fitted vectorizer\n",
    "    \"\"\"\n",
    "    if vectorizer==None:\n",
    "        vectorizer = TfidfVectorizer(ngram_range=(1,2), max_features=100000)\n",
    "        vectorizer.fit(col_data)\n",
    "    ohe_data = vectorizer.transform(col_data)\n",
    "    return ohe_data, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aece52e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_pipeline(X_data, general_cat_vectorizer=None, subcat_1_vectorizer=None, subcat_2_vectorizer=None, brand_name_vectorizer=None, item_name_vectorizer=None, \n",
    "                     item_desc_vectorizer=None, scaler_name_len=None, scaler_desc_len=None):\n",
    "    \"\"\"\n",
    "    Description: This function will do all the feature engineering on the input X_data,\n",
    "                and create a final data, ready for training.\n",
    "\n",
    "    Input: Original input dataframe, \n",
    "                    the fitted vectorizers for all categorical and text columns [optional: used during inference],\n",
    "                    scalers [optional: used during inference]\n",
    "    Output: Featurized data\n",
    "    \"\"\"\n",
    "    print()\n",
    "    print(\"Filling missing values...\")\n",
    "    X_data = fill_missing_values(X_data)\n",
    "\n",
    "\n",
    "    print(\"pre-processing text data...\")\n",
    "    X_data['item_description'] = X_data['item_description'].apply(preprocess_descriptive_text_column) ### temp step ###\n",
    "    X_data['name'] = X_data['name'].apply(preprocess_descriptive_text_column)                         ### temp step ###\n",
    "    X_data['brand_name'] = X_data['brand_name'].apply(lambda x: str(x).lower())\n",
    "\n",
    "    print(\"Guessing the missing brands...\")\n",
    "    X_data = brand_guesser(X_data)\n",
    "\n",
    "   \n",
    "    print('Getting word lengths')\n",
    "    name_len, scaler_name_len =  get_len_feature(X_data['name'], scaler_name_len)\n",
    "    desc_len, scaler_desc_len =  get_len_feature(X_data['item_description'], scaler_desc_len)\n",
    "    \n",
    "    print(\"Splitting categories...\")\n",
    "    X_data = split_categories(X_data)\n",
    "    \n",
    "    print(\"Getting is_expensive brand feature...\")\n",
    "    sparse_is_expensive = get_is_expensive_feature(X_data)\n",
    "\n",
    "    print(\"Getting sparse shipping data...\")\n",
    "    sparse_shipping = get_shipping_feature(X_data)\n",
    "\n",
    "    print(\"OHE vectorizing the text and categorical variables...\")\n",
    "    general_cat_ohe, general_cat_vectorizer = vectorize_data(X_data['general_cat'].values.astype('U'), general_cat_vectorizer)\n",
    "    print(\"general cat done...\")\n",
    "    subcat_1_ohe, subcat_1_vectorizer = vectorize_data(X_data['subcat_1'].values.astype('U'), subcat_1_vectorizer)\n",
    "    print(\"sub cat 1 done...\")\n",
    "    subcat_2_ohe, subcat_2_vectorizer = vectorize_data(X_data['subcat_2'].values.astype('U'), subcat_2_vectorizer)\n",
    "    print(\"sub cat 2 done...\")\n",
    "    brand_name_ohe, brand_name_vectorizer = vectorize_data(X_data['brand_name'].values.astype('U'), brand_name_vectorizer)\n",
    "    print(\"brand name done...\")\n",
    "    item_name_ohe, item_name_vectorizer = vectorize_data(X_data['name'], item_name_vectorizer)\n",
    "    print(\"item name done...\")\n",
    "    item_desc_ohe, item_desc_vectorizer = vectorize_data(X_data['item_description'], item_desc_vectorizer)\n",
    "    print(\"item description done...\")\n",
    "\n",
    "    print(\"Creating the final featurized dataset...\")\n",
    "    X_featurized = hstack((general_cat_ohe, subcat_1_ohe, subcat_2_ohe, brand_name_ohe, item_name_ohe, item_desc_ohe, \n",
    "                            desc_len, name_len, X_data['item_condition_id'].values.reshape(-1,1), sparse_shipping)).tocsr()\n",
    "\n",
    "    print(\"Done!!!\\n---------------------------\\n\")\n",
    "    return X_featurized, general_cat_vectorizer, subcat_1_vectorizer, subcat_2_vectorizer, brand_name_vectorizer, item_name_vectorizer, item_desc_vectorizer, scaler_name_len, scaler_desc_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4945f7b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filling missing values...\n",
      "pre-processing text data...\n",
      "Guessing the missing brands...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79fecfc3c30542ad9394601d3697f0c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54720 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting word lengths\n",
      "Splitting categories...\n",
      "Getting is_expensive brand feature...\n",
      "Getting sparse shipping data...\n",
      "OHE vectorizing the text and categorical variables...\n",
      "general cat done...\n",
      "sub cat 1 done...\n",
      "sub cat 2 done...\n",
      "brand name done...\n",
      "item name done...\n",
      "item description done...\n",
      "Creating the final featurized dataset...\n",
      "Done!!!\n",
      "---------------------------\n",
      "\n",
      "\n",
      "Filling missing values...\n",
      "pre-processing text data...\n",
      "Guessing the missing brands...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be8a8549713b42af9fbb02f5d6d8d020",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13680 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting word lengths\n",
      "Splitting categories...\n",
      "Getting is_expensive brand feature...\n",
      "Getting sparse shipping data...\n",
      "OHE vectorizing the text and categorical variables...\n",
      "general cat done...\n",
      "sub cat 1 done...\n",
      "sub cat 2 done...\n",
      "brand name done...\n",
      "item name done...\n",
      "item description done...\n",
      "Creating the final featurized dataset...\n",
      "Done!!!\n",
      "---------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_tr, general_cat_vectorizer, subcat_1_vectorizer, subcat_2_vectorizer, brand_name_vectorizer, item_name_vectorizer, item_desc_vectorizer, scaler_name_len, scaler_desc_len = feature_pipeline(X_train)\n",
    "X_te, _, _, _, _, _, _, _, _ = feature_pipeline(X_test, general_cat_vectorizer, subcat_1_vectorizer, subcat_2_vectorizer, brand_name_vectorizer, item_name_vectorizer, item_desc_vectorizer, scaler_name_len, scaler_desc_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f933266e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_log_error\n",
    "def evaluate_model(model, X_data, y_act):\n",
    "    y_pred = model.predict(X_data)\n",
    "\n",
    "    act_prices = np.round(np.exp(y_act), 2)\n",
    "    pred_prices = np.round(np.exp(y_pred), 2)\n",
    "\n",
    "    rms = np.sqrt(mean_squared_log_error(act_prices.values, pred_prices))\n",
    "    return rms\n",
    "\n",
    "def do_inference(model, X_te):\n",
    "  y_pred = model.predict(X_te)\n",
    "  pred_prices = np.round(np.exp(y_pred), 2)\n",
    "\n",
    "  return pred_prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6fee8b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal alpha: 1.0\n",
      "RMSLE: 0.4982038308540824\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "ridge_cv = RidgeCV(alphas=(0.01, 0.1, 1.0, 10.0), cv=3)\n",
    "ridge_cv.fit(X_tr, y_train)\n",
    "rmsl_error = evaluate_model(ridge_cv, X_te, y_test)\n",
    "print(\"Optimal alpha:\",ridge_cv.alpha_)\n",
    "print(\"RMSLE:\",rmsl_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2f883e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2762801533459403\n"
     ]
    }
   ],
   "source": [
    "rmsl_error = evaluate_model(ridge_cv, X_tr, y_train)\n",
    "print(rmsl_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f1f06f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<13680x165012 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 416503 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13c813af",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [['coin necklac',1,'Women/Jewelry/Necklaces','forever 21',0,2982673593,'silver','Women','Jewelry','Necklaces']]\n",
    "dftest = pd.DataFrame(data, columns = ['name', 'item_condition_id','category_name','brand_name','shipping','seller_id','item_description','general_cat','subcat_1','subcat_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d43c49c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>item_condition_id</th>\n",
       "      <th>category_name</th>\n",
       "      <th>brand_name</th>\n",
       "      <th>shipping</th>\n",
       "      <th>seller_id</th>\n",
       "      <th>item_description</th>\n",
       "      <th>general_cat</th>\n",
       "      <th>subcat_1</th>\n",
       "      <th>subcat_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>coin necklac</td>\n",
       "      <td>1</td>\n",
       "      <td>Women/Jewelry/Necklaces</td>\n",
       "      <td>forever 21</td>\n",
       "      <td>0</td>\n",
       "      <td>2982673593</td>\n",
       "      <td>silver</td>\n",
       "      <td>Women</td>\n",
       "      <td>Jewelry</td>\n",
       "      <td>Necklaces</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           name  item_condition_id            category_name  brand_name  \\\n",
       "0  coin necklac                  1  Women/Jewelry/Necklaces  forever 21   \n",
       "\n",
       "   shipping   seller_id item_description general_cat subcat_1   subcat_2  \n",
       "0         0  2982673593           silver       Women  Jewelry  Necklaces  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c201f5d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filling missing values...\n",
      "pre-processing text data...\n",
      "Guessing the missing brands...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a91fa3519fe041ebb6d086fbc041f284",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting word lengths\n",
      "Splitting categories...\n",
      "Getting is_expensive brand feature...\n",
      "Getting sparse shipping data...\n",
      "OHE vectorizing the text and categorical variables...\n",
      "general cat done...\n",
      "sub cat 1 done...\n",
      "sub cat 2 done...\n",
      "brand name done...\n",
      "item name done...\n",
      "item description done...\n",
      "Creating the final featurized dataset...\n",
      "Done!!!\n",
      "---------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Xtest, general_cat_vectorizer, subcat_1_vectorizer, subcat_2_vectorizer, brand_name_vectorizer, item_name_vectorizer, item_desc_vectorizer, scaler_name_len, scaler_desc_len = feature_pipeline(dftest)\n",
    "Xtetest, _, _, _, _, _, _, _, _ = feature_pipeline(dftest, general_cat_vectorizer, subcat_1_vectorizer, subcat_2_vectorizer, brand_name_vectorizer, item_name_vectorizer, item_desc_vectorizer, scaler_name_len, scaler_desc_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3031ded6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10199    1.609438\n",
       "26198    2.302585\n",
       "65487    3.912023\n",
       "3617     5.298317\n",
       "21906    2.197225\n",
       "           ...   \n",
       "37194    2.708050\n",
       "6265     1.098612\n",
       "54886    3.218876\n",
       "860      4.605170\n",
       "15795    1.791759\n",
       "Name: log_price, Length: 54720, dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c7bfb11d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x165012 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 13 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtetest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "843ff7fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.18044726])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.expm1(ridge_cv.predict(Xtetest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8f604b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filling missing values...\n",
      "pre-processing text data...\n",
      "Guessing the missing brands...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "109b5d4230b6457baf650f91a5bcd8b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31789 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting word lengths\n",
      "Splitting categories...\n",
      "Getting is_expensive brand feature...\n",
      "Getting sparse shipping data...\n",
      "OHE vectorizing the text and categorical variables...\n",
      "general cat done...\n",
      "sub cat 1 done...\n",
      "sub cat 2 done...\n",
      "brand name done...\n",
      "item name done...\n",
      "item description done...\n",
      "Creating the final featurized dataset...\n",
      "Done!!!\n",
      "---------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv('mercari_test.csv')\n",
    "Xtestsparse, _, _, _, _, _, _, _, _ = feature_pipeline(test, general_cat_vectorizer, subcat_1_vectorizer, subcat_2_vectorizer, brand_name_vectorizer, item_name_vectorizer, item_desc_vectorizer, scaler_name_len, scaler_desc_len)\n",
    "submission = test[['id']]\n",
    "import pickle\n",
    "with open('price_prediction_promising.pickle','wb') as f:\n",
    "    pickle.dump(ridge_cv,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702612bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtestsparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48aaa07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = ridge_cv.predict(Xtestsparse)\n",
    "\n",
    "submission[\"price\"] = np.expm1(preds)\n",
    "submission.to_csv(\"submission_ridgecv_promising.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d4d756",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "columns = {\n",
    "    'data_columns' : [col.lower() for col in X.columns]\n",
    "}\n",
    "with open(\"columns.json\",\"w\") as f:\n",
    "    f.write(json.dumps(columns))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
